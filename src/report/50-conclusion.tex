\chapter*{\hfill{\centering ЗАКЛЮЧЕНИЕ }\hfill}
\addcontentsline{toc}{chapter}{ЗАКЛЮЧЕНИЕ}


В ходе выполнения данной работы были выполнены следующие задачи:

\begin{itemize}[label=---]
    \item задача была формализована используя игры маркова;
    \item был проведден анализ предметной области;
    \item сформулированы способы классификации и сравнения методов;
    \item методы классифицированы по предложенным способам;
    \item проведено сравнение алгоритмов;
    \item результат сравнения алгоритмов отражен в выводе.
\end{itemize}

Современные алгоритмы классического обучения (IPPO, IA2C) с подкреплением показывают хорошие результаты в среде с несколькими агентами.
Тем не менее, использование централизованной Q--сети улучшает резултаты во всех случаях. Подобные алгориты, основанные а методе Actor-Critic 
уместнее применять к средам с континуальным действием. 

В среде с дискретным действием, таких как данная, лучше использовать алгоритмы без обучения страгении (Policy Learning), по типу MAVEN, IQL.

Если среда обладает большим пространством действий, то лучше использовать MAVEN, т. к. он гарантирует консистентное исследование среды.
Он также показывает лучшую сходимость в других средах.

